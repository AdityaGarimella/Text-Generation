# Text-Generation
Dataset Preparation - Tokenising to extracttokens (terms / words) from the corpus using keras inbuilt model.Converting the corpus tokens the corpus into a flat dataset of sentence sequences. It is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose.To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label.Data Modelling - Using these Input vectors and Input labels for training a keras sequential LSTM model.Generating Text - We tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.
